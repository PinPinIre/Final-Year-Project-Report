\chapter{}

In this section the background to the state of the art in machine learning and recomdation systems will be presented.
In particular the use of machine learning algorithms used as the content filtering component of recomendation systems will be discussed.
The different approaches to machine learning and recomendation systems will also be introduced and then discussed in detail.

\section{Background}
In 1947 Alan Turing presented a lecture to the London Mathematical Society in which he theorised that it would be possible for a machine to learn from it's experiences.
In Turing's example he proposed that learning be a prerequiste for a true intelligent system \cite{Turing1946}.
Turing further expanded on the concept of an intelligent machine in his 1950 paper which proposed a theorethical test, which he called the "imitation game", to identify whether machines could be considered intelligent \cite{Turing1950}.
This test has since become know as the Turing test.

Machine learning and recomendation systems can be viewed as semi-intelligent machines which try to learn from data fed into them.
These machines are replacements for human operators who would have had to classify data or suggest recomendation in the past.
One of main reasons machines are used instead of humans for this task is largely due to the volume of data available.
Another factor to consider is the quality of results obtained from the system.
For example humans can be biased by their opinion when presenting their results or findings.
A machine on the other hand generates it's results using the model it has created using the data that has been presented to it.
This approach is less likely to be skewed by personal bias when the model has been created using a balanced dataset\cite{FProvost2000}.

\section{Machine Learning}
Machine learning is an interdisiplinary field concerned with the study of self learning systems.
Machine learning has applications in fields such as; statistics, mathmathics, computer vision, game theory, information retrival, software engineering, sentiment analysis, artifical intelligence.

Machine learning algorithms try to build a model of a dataset by learning the patterns in the data.
The result of this is that machine learning algorithms can output seeminly intelligent results for data it has never seen before.

Machine learning can be split into three different types of learning:
\begin{itemize}
    \item Supervised learning
    \item Reinforced learning
    \item Unsupervised learning
\end{itemize}

\subsection{Supervised learning}
Supervised learning is a machine learning technique which tries to create a model which maps inputs to desired outputs.
This can be represented as a function f(x) which maps an input \(x_i\) to an output \(y_i\).
This is acheived by using a training set T = \((x_i, y_i)\) and a learning algorithm.
The learning algorithm produces a function \(\hat{f}(x_i)\) which can then be modified in response to the difference between \(y_i - \hat{f}(x_i)\).

\subsection{Reinforced learning}
Reinforced learning is a machine learning technique where the machine interacts with an environment and produces actions \(a_i\).
These set of actions interact with the environment which in turn results in the machine receiving rewards \(r_i\) from a rewards function.
The machine tries to learn how to create actions which maximises the future return on rewards.

\subsection{Unsupervised learning}
Unsupervised learning is a machine learning technique which tries to find hidden patterns in data.
An unsupervised learning algorithm receives inputs \(x_i\) but receives no desired outputs nor rewards.
The machine the tries to build a probalistic model of the data or it uses clustering to partion the data in categories.

The main difference between unsupervised techniques and other techniques is the lack of a clear measure of success.
This poises a problem when comparing the accuracy of different unsupervised techniques.
The effectivness of unsupervised techniques therefore rely heavily on heuristic approaches when judging their quality.

The research conducted in this project is focused on this type of machine learning.
Due to the fact that there is no gold standard to compare the quality of the similarity results, their effectiveness will be a matter of opinion.
Where possible the results will also be compared against any possible meta-data to help judge their effectivness.

\section{Recomendation Systems}
Recomendation systems are algorithms and technniques which try to generate recomenations for users.
The design of recomendation systems is an interdisiplinary field which touches upon information retrival, human computer interaction, machine learning, data mining, etc.
These systems often try to generate recomendations based on similarities between users or between content.
There are two main approaches used in recomendation systems;

\begin{itemize}
    \item Collaborative-filtering
    \item Content-filtering
\end{itemize}

\subsection{Collaborative-filtering}
Collaborative-filtering is an approach used by recomendation systems which tries to generate recomendations by comparing user data with other users.
The rationale behind this approach is to find similarities and differneces  between different groups of users and build a user model.
Once users have been grouped together recomendations can be generated based on what similar users have liked and disliked.
This type of recomendation system has been used extensively by Amazon when generating recomdations.

The main problem with this approach is that it requires a huge amount of inital data in order to generate appropriate recomendations.

\subsection{Content-filtering}
Content-filtering is an approach used by recomdation systems which learns to generate recomendations based on a users previous interations with an item.
This approach tries to learn the features of items in order to generate recomdations for items with similar features.

Unlike collaborative-filtering a huge amount of user data is not required for content-filtering.
The recomdations for this approach can be generated by building a model using the individual item features.

The research conducted in this project is focused entirely on the information filtering component of a content-filtering recomendation system.
The research project conducted was based on a potential user being a student.
However all of the algorithms investigated could be used in an content-filtering recomdation systems to filter recomendations based on the contenc of the items.

\section{State of the Art}
This research project is investigating the performance of LDA, k-NN and Word2Vec as the content-filtering component of a recomendation system.
Each of these algorithms have applications in information retrival and are not just confined to the field of recomendations systems nor an educational corpus.

\subsection{Latent Dirichlet Allocation}
LDA is a probalistic model for any discreate data but in this research project and most of the literature reviewed, textual data has been used.
LDA was introduced in 2004 by Blei, Ng and Jordan as an improvement on Deerwester's Latent Semantic Indexing (LSI) and on Hofmann's later Probalistic Latent Semantic Indexing (pLSI).
LDA was created in response to two problems identified with pLSI; the number of paramaters in the model grew linearly with the size of the corpus, and it was unclear as to how to assign a probability to a document that the model had never seen before.

LDA is hierarchical model which assumes that documents contain a mixture of topics and that topics are a mixture of word probabilities.
LDA is also a bag of words model which assumes that the topics are generated first and then documents are generated from these topics.
These assumptions are used when infering topics as it a reversing of the generation process.

LDA has been used in the past on a corpus of 17,000 articles from the magazine Science, with 100 topics.
The LDA model that was generated using this corpus was fed an unseen article on genome mapping and sequencing.
The distibution of topics in the article were then calculated and graphed.
It was found that the topics which seemed to be about; genetics, evoloution, disease, and computers had a high concentration in the article.

In a similar study to the above, LDA was applied to 21,434 articles from Science to create 50 topics.
The purpose of this experiment was to infer the most relevant topics from a document and then to find the most similar documents.
A article on Statistical Significance in Protein and DNA sequencing was used to generate recomendations for articles related to Protein sequencing, genome sequencing and sampling strategies.

\subsection{k-Nearest-Neighbours}
k-Nearest-Neighbours is a model free algorithm for classification and pattern recognition.
In k-Nearest-Neighbour classifiers when given a query point \textit{x}, the k nearest points in distance to \textit{x} are used to classify x.
When k is 1, the query point x is classified as being the same as the point nearest to it.
When k is a value greater than 1 a simple majority vote of the k nearest points can be used to classify \textit{x}\cite{elementsStat}.

When k-Nearest-Neighbours is applied to high-dimensional feature space the distance between the k nearest neighbours can be quite high causing bias and a degredation of performance\cite{elementsStat}.



\subsection{Word2Vec}

\section{Conclusion}
